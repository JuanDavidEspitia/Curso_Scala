{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\scala-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de Datos Distribuidos Resilientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que es un conjunto de datos?\n",
    "Un conjunto de datos es basicamente una coleccion de datos; puede ser una lista de strings, una lista de enteros e incluso\n",
    "el numero de filas de una base de datos relacional. Pueden contener cualquier tipo de objetos, incluidas clases definidas por el usuario\n",
    "Un RDD es simplemente una compilacion alrededor de un gran conjunto de datos. En Spark todo el trabajo esta expresado ya sea \n",
    "creando nuevas RDDs, transformando RDDs existentes, o ejecutando operaciones en RDDs para calcular un resultado.\n",
    "Spark se encarga de distribuir automaticamente los datos contenidos en RDDs a travez de tu cluster y paralelizara las opreaciones que realices en ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ¿Que podemos hacer con los RDDs?\n",
    "1. Transformaciones: Aplican basicamente algunas funciones a los datos de un RDD para crear un nuevo RDD\n",
    "  Una de las transformaciones mas comunes es filterla cual genera un nuevo RDD que contiene un subconjunto de datos de un RDD     existente\n",
    "  \n",
    "2. Accioes: Calcula un resultado basado en un RDD\n",
    "   Una de las acciones mas populares es first, la cual retornara el primer elemento de un RDD existente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo de Trabajo de Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Genererará RDDs iniciales a partir de datos externos\n",
    "- Aplicara transformaciones\n",
    "- Iniciara acciones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion de RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creacion de RDDs se puede dar de dos maneras:\n",
    "1. Cargando una BD externa\n",
    "2. Paralelizando una coleccion en el programa driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Haciendo uso del primer metodo \n",
    "val inputInt = List(1,2,3,4,5)\n",
    "val intRDD = sc.parallelize(inputInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma comun de trabajar con RDDs es:\n",
    "Carga los RDDs desde un almancenamiento externo usando el metodo textfile en SparkContext"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val sc = new SparkContext(conf)\n",
    "val lines = sc.textFile(\"input/uppercase.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones Map y Filter en Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son operaciones sire RDDs las cuales generan como resultado un nuevo RDD.\n",
    "Las dos transformaciones mas comunes son filter y map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuta como una funcion y genera un RDD formado por aquellos elementos seleccionados sobre los cuales se ejecuta la funcion filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ser usada para eliminar algunas filas invalidas, para limpiar el RDD input o solo para conseguir un sobconjunto del RDD basado en la funcion filtro"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val cleanRDD = rdd.filter(line => !line.isEmpty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuta como una funcion sobre cada elemento del RDD input y esta funcion genera un nuevo valor para cada elemento los cuales se almacena en el RDD Resultante "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ser usada para crear solicitudes HTTP para cada URL contenida en nuestro RDD input, o puede ser usada para calcular la raiz cuadrada de cada numero"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ejemplo\n",
    "\n",
    "val urls = sc.textFile(\"input/urls.txt\")\n",
    "urls.map(url => makeHttpRequest(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tipo de valores como resultado de ejecutar la funcion Map no son necesariamente los mismos del RDD input"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val lines = sc.textFile(\"input/uppercase.text\")\n",
    "val lengths = lines.map(line => line.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un programa Spark para leer los datos del aeropuerto desde / airports.text, encuentre todos los aeropuertos ubicados en Estados Unidos y enviar el nombre del aeropuerto y el nombre de la ciudad a / airports_in_usa.text. Cada fila del archivo de entrada contiene las siguientes columnas:  Identificación del aeropuerto, nombre del aeropuerto, ciudad principal a la que sirve el aeropuerto, país donde se encuentra el aeropuerto, código IATA / FAA,  Código OACI, latitud, longitud, altitud, zona horaria, horario de verano, zona horaria en formato Olson."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ejemplo de Salida:\n",
    "\n",
    "\"Putnam County Airport\", \"Greencastle\"\n",
    "\"Dowagiac Municipal Airport\", \"Dowagiac\"\n",
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "\n",
    "val airports = sc.textFile(\"input/airports.text\")\n",
    "val airportsInUSA = airports.filter(line => line.split(\",\")(3) == \"\\\"United States\\\"\")\n",
    "\n",
    "val airportsNameAndCityNames = airportsInUSA.map(line => {\n",
    "      val splits = line.split(\",\")\n",
    "      splits(1) + \", \" + splits(2)\n",
    "    })\n",
    "    airportsNameAndCityNames.saveAsTextFile(\"out/airports_in_usa.text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un programa Spark para leer los datos del aeropuerto desde / airports.text, encuentre todos los aeropuertos cuya latitud sea mayor que 40. Luego, envíe el nombre del aeropuerto y la latitud del aeropuerto a / airports_by_latitude.text.\n",
    "Cada fila del archivo de entrada contiene las siguientes columnas: Identificación del aeropuerto, nombre del aeropuerto, ciudad principal a la que sirve el aeropuerto, país donde se encuentra el aeropuerto, código IATA / FAA, Código OACI, latitud, longitud, altitud, zona horaria, horario de verano, zona horaria en formato Olson"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ejemplo de salida:\n",
    "\n",
    "\n",
    " \"St Anthony\", 51.391944\n",
    " \"Tofino\", 49.082222\n",
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "\n",
    "val airports = sc.textFile(\"input/airports.text\")\n",
    "val airportsInUSA = airports.filter(line => line.split(\",\")(6).toFloat > 40)\n",
    "\n",
    "val airportsNameAndCityNames = airportsInUSA.map(line => {\n",
    "val splits = line.split(\",\")\n",
    "     splits(1) + \", \" + splits(6)\n",
    "    })\n",
    " //   airportsNameAndCityNames.saveAsTextFile(\"out/airports_by_latitude.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformacion FlatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera un nuevo RDD al primero aplicar una funcion a todos los elementos de este RDD y luego aplanando los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio:\n",
    "WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.Level\n",
    "import org.apache.log4j.Logger\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark._  \n",
    "   \n",
    "   Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "    val lines = sc.textFile(\"input/word_count.text\")\n",
    "    val words = lines.flatMap(line => line.split(\" \"))\n",
    "\n",
    "    val wordCounts = words.countByValue()\n",
    "   // countByValue() nos dara como resutlado la cantidad de cada valor unico del RDD \n",
    "    for ((word, count) <- wordCounts) println(word + \" : \" + count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las operaciones Set son realizadas en un RDD\n",
    " - Sample\n",
    " - Distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La operacion Sample creara un ejemplo al azar a partir de un RDD\n",
    "- Muy practico para realizar pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provee un nuevo RDD conteniendo los elementos distinct en este RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La transformacion retornara unicamente filas que sean distintas a partir de un RDD input\n",
    "- La operacion distinct es muy costosa por que requiere combinar todos los datos a travez de particiones para asegurar que recibiremos una sola copia de cada elemento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones Set que se ejecutan sobre dos RDDs y producen un RDD resultante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Union\n",
    "- Intersection\n",
    "- Substract\n",
    "- Cartesian product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operacion de conjuntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\operaciones-conjuntos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producto cartesiano\n",
    "\n",
    "Las transformaciones cartesian genera todos los posibles emparejamientos de A y B donde A y B son fuentes que corresponden a cada RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\product_cartesian.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo \"in / nasa_19950701.tsv\" contiene 10000 líneas de registro de uno de los servidores apache de la NASA para el 1 de julio de 1995.\n",
    "El archivo \"in / nasa_19950801.tsv\" contiene 10000 líneas de registro para el 1 de agosto de 1995\n",
    "Cree un programa Spark para generar un nuevo RDD que contenga las líneas de registro tanto del 1 de julio como del 1 de agosto,\n",
    "tome una muestra de 0.1 de esas líneas de registro y guárdela en el archivo \"out / sample_nasa_logs.tsv\".\n",
    " Tenga en cuenta que los archivos de registro originales contienen las siguientes líneas de encabezado. host logname tiempo método url respuesta bytes\n",
    " Asegúrese de que las líneas principales se eliminen en el RDD resultante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._ \n",
    "\n",
    "val julyFirstLogs = sc.textFile(\"input/nasa_19950701.tsv\")\n",
    "val augustFirstLogs = sc.textFile(\"input/nasa_19950801.tsv\")\n",
    "// Realizamos la union de los dos archivos de texto\n",
    "val aggregatedLogLines = julyFirstLogs.union(augustFirstLogs)\n",
    "\n",
    "// Imprimimos las 10  primeras filas del RDD\n",
    "aggregatedLogLines.take(10).foreach(println)\n",
    "\n",
    "//Contamos la cantidad de registros del RDD\n",
    "aggregatedLogLines.count()\n",
    "\n",
    "// Podemos tambien borrar los encabezados del RDD\n",
    "aggregatedLogLines.collect().drop(1)\n",
    "\n",
    "// Limpiamos quitando los encabezados\n",
    "//val cleanLogLines = aggregatedLogLines.filter(line => isNotHeader(line))\n",
    "\n",
    "//val sample = cleanLogLines.sample(withReplacement = true, fraction = 0.1)\n",
    "\n",
    "// Por ultimo guardamos el RRD resultante como archivo de texto\n",
    "//sample.saveAsTextFile(\"out/sample_nasa_logs.csv\")\n",
    "// }\n",
    "\n",
    "//def isNotHeader(line: String): Boolean = !(line.startsWith(\"host\") && line.contains(\"bytes\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las acciones son las que generan un valor final al programa driver o mantienen los datos de un sistema de almacenamiento interno\n",
    "- LAs acciones obligaran la evaluacion de las transformaciones requeridas para el RDD al que  fueron llamadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acciones mas Populares:\n",
    "- collect\n",
    "- count\n",
    "- countByValue\n",
    "- take\n",
    "- saveAsTextFile\n",
    "- reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La operacion collect obtiene el RDD entero y lo genera en el programa driver en forma de una coleccion regular o valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si tienes un string RDD, cuando utilizas una accion en el, conseguiras una lista de string\n",
    "- Es muy util si el programa spark tiene RDDs filtrados por debajo de un tamaño relativamente pequeño y deseas lidiar con ellos localmente\n",
    "- La base de datos entera debe ajustarse en la memoria en una sola maquina ya que todo lo que necesitas es ser copiada la driver cuando la accion collect es utilizada. Entonces la collect NO deberia ser usada en grandes bases de datos\n",
    "- La operacion Collect es ampliamente usada en pruebas unitarias para comparar el valor de nuestro RDD cn nuestro resultado esperado, siempre y cuando todo el contenido del RDD se ajuste en la memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputWords = List(\"spark\", \"hadoop\", \"spark\", \"hive\", \"pig\", \"cassandra\", \"hadoop\")\n",
    "val wordRdd = sc.parallelize(inputWords)\n",
    "\n",
    "val words = wordRdd.collect()\n",
    "\n",
    "for (word <- words) println(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count & ContByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si quieres contar el numero de filas en un RDD, la operacion count es una manera rapida de hacerlo. Esta genera el contero de los elementos\n",
    "\n",
    "- countByValue buscara valores unicos en cada fila de tu RDD y generara un mapeo de cada valor unico con su conteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputWords = List(\"spark\", \"hadoop\", \"spark\", \"hive\", \"pig\", \"cassandra\", \"hadoop\")\n",
    "val wordRdd = sc.parallelize(inputWords)\n",
    "println(\"Count: \" + wordRdd.count())\n",
    "\n",
    "println(\"------------------------------------\")\n",
    "\n",
    "val wordCountByValue = wordRdd.countByValue()\n",
    "println(\"CountByValue:\")\n",
    "for ((word, count) <- wordCountByValue) println(word + \" : \" + count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La accion take toma n cantidad de elemenetos de un RDD\n",
    "- La operacion take puede ser muy util para realizar pruebas unitarias y uyna rapida depuracion\n",
    "- take genera n elementos apartir de un RDD y esta intentara reducir el numero de particiones a las que accede, entonces es posible que la operacion take termine dandonos una coleccion aleatoria y no necesariamente generar los elementos en el orden que podriamos esperear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputWords = List(\"spark\", \"hadoop\", \"spark\", \"hive\", \"pig\", \"cassandra\", \"hadoop\", \"hive\", \"Hbase\", \"Sqoop\")\n",
    "val wordRdd = sc.parallelize(inputWords)\n",
    "\n",
    "val words = wordRdd.take(3)\n",
    "for (word <- words) println(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SaveAsTextFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SaveAsTextFile puede ser usado para escribi datos de salida a sistemas de almacenamiento distribuidos como HDFS o Amazon S3, o inclusive a archivos del sistema local"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ejemplo\n",
    "\n",
    "// Por ultimo guardamos el RRD resultante como archivo de texto\n",
    "sample.saveAsTextFile(\"out/sample_nasa_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce toma una funcion que opera sobre dos elementos que hay en el RDD de entrada y devuelve un Nuevo elemento del mismo tipo. Esto reduce los elementos de este RDD usando la funcion binaria especificada.\n",
    "- Esta funcion produce el mismo resultado cuando se aplica de forma repetitiva en el mismo conjunto de datos RDD y se reduce a un solo valor\n",
    "- Con la operacion Reduce podemos facilmente sumar todos los elementos de un RDD, contar el numero total de elementos o realizar otros tipos de operaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputIntegers = List(1, 2, 3, 4, 5)\n",
    "val integerRdd = sc.parallelize(inputIntegers)\n",
    "\n",
    "val product = integerRdd.reduce((x, y) => x * y)\n",
    "println(\"product is :\" + product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un programa Spark para leer los primeros 100 números primos en / prime_nums.text, imprime la suma de esos números a la consola. Cada fila del archivo de entrada contiene 10 números primos separados por espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = sc.textFile(\"input/prime_nums.text\")\n",
    "\n",
    "val numbers = lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "\n",
    "val validNumbers = numbers.filter(number => !number.isEmpty)\n",
    "\n",
    "val intNumbers = validNumbers.map(number => number.toInt)\n",
    "\n",
    "println(\"Sum is: \" + intNumbers.reduce((x, y) => x + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspectos importantes de RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDDs son distribuidos: se divide en partes llamadas particiones y cada particion se divide a traves del cluster\n",
    "- Es proceso de particionado se realiza automaticamente por Spark, lo que no necesitas preocuparte por todos los detalles acerca de como se dividen sus datos a traves del cluster.\n",
    "- Los RDD son inmutables: No pueden ser cambiados despues de ser creados\n",
    "- Los RDD son resilientes: son una funcion deterministica de sus datos de entrada. Esto mas inmutabilidad tambien significa que ciertas partes de los RDDs se pueden crecrear en cualquier momento.\n",
    "- En caso de que un nodo del cluster se caiga, Spark puede recuperar las partes de los RDDs de los datos de entrada y recogerlos desde donde quedaron.\n",
    "- Son Tolerantes a fallos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resumen: Las tranasformaciones generan RDDs, mientras que las acciones generan totalmente algun otro tipo de dato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento en Cache y Persistencia (Caching y Persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algunas veces nos gustaria utilizar acciones sobre el mismo RDD muchas veces\n",
    "- Si hacemos esto sin preocupacion, los RDD y todos sus componentes seran recalculados cada vez que una accion sea utilizada sobre un RDD\n",
    "- Esto puede ser muy costoso, especialmente por algunos algoritmos iterativos los cuales utilizan acciones en la misma base de datos varias veces.\n",
    "- Si quieres reusar un RDD en multiples acciones, puedes tambien pedir a Spark que persista mediante el uso del metodo persist() sobre el RDD.\n",
    "- Cuando persistes un RDD, la primera vez es calculando en una accion este se mantendra en la memoria a traves de los nodos\n",
    "- Permite que las acciones futuras sean mas rapidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val inputIntegers = List(1, 2, 3, 4, 5)\n",
    "val integerRdd = sc.parallelize(inputIntegers)\n",
    "\n",
    "integerRdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "integerRdd.reduce((x, y) => x * y)\n",
    "integerRdd.count()\n",
    "\n",
    "//RDD.cache() = RDD.persist(MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura Spark (Maestro - Esclavo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\spark-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura de la computación en clúster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\spark-architecture-II.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\components-spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Core proporciona las APIs necesarias para la manitpulacion de estas colecciones y permite la ejecucion de modelos, Tareas y Almacenamiento en cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
