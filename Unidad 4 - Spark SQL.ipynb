{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion a Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\spark-sql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la interfaz de Spark para trabajar con datos estructurados y semiestructurados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datos estructurados son todos aquellos datos que presentan un esquema, es decir, existe un conjunto de campos para cada registro.\n",
    "- Spark SQL proporciona un conjunto de datos abstractos que simplifica el trabajo con conjuntos de datos estructurados. El conjunto de datos es similar a las tablas de una base de datos relacional.\n",
    "- Cada vez mas el flujo de trabajo de Spark se mueve hacia Spark SQL\n",
    "- El principal objetivo es trabajar con datos estructurados.\n",
    "- Un conjunto de datos tiene un esquema predeterminado, y esto permite que Spark almacene informacion de una manera mas eficiente y pueda ejecutar consultas SQL sobre estos datos utilizando comandos SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptos importantes de Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataframes\n",
    "#### 2. Conjuntos de datos (Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark SQL introduce una abstraccion de datos tabulares llamada **Dataframe** a partir de la version 1.3.\n",
    "- Un Dataframe es una abstraccion de datos o un lenguaje especifico de dominio para trabajar con datos estructurados o semiestructurados\n",
    "- Los Dataframes almacenan datos de una manera mas eficiente comparados con RDDs nativos, aprovechando su esquema\n",
    "- Utiliza las capacidades inmutables, en memoria, resilentes, distribuidas y paralelas de un RDD y aplica una estructura llamada esquema a los datos, permitiendo asi que Spark administre esta estrucutra y solo pase datos entre nodos de una manera mucho mas eficiente en lugar de la serializacion de objetos en Java\n",
    "- A diferencia de un RDD, la informacion es organizada en columnas, con nombres especificos similar a una tabla en una Base de datos Relacional\n",
    "\n",
    "\n",
    "![title](images\\dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjuntos de Datos (Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El API de los conjuntos de datos, presente desde la version 1.6 de Spark proporciona:\n",
    "- El familiar estilo de programacion orientado a objetos\n",
    "- El Compile-Time y Type Safety de la API del RDD\n",
    "- El beneficio de usar los sistemas para trabajar con datos estructurados\n",
    "\n",
    "Un Conjunto de datos generalmente es un conjunto de datos estructurados, no necesesariamente una fila pero podria ser de tipo particular\n",
    "\n",
    "\n",
    "![title](images\\dataset-dataframe-rdd.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://9HQ2BL2.itcol.com:4042\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1596723835586)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Print out schema ===\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- collector: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- un_subregion: string (nullable = true)\n",
      " |-- so_region: string (nullable = true)\n",
      " |-- age_range: string (nullable = true)\n",
      " |-- age_midpoint: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- self_identification: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- occupation_group: string (nullable = true)\n",
      " |-- experience_range: string (nullable = true)\n",
      " |-- experience_midpoint: double (nullable = true)\n",
      " |-- salary_range: string (nullable = true)\n",
      " |-- salary_midpoint: double (nullable = true)\n",
      " |-- big_mac_index: double (nullable = true)\n",
      " |-- tech_do: string (nullable = true)\n",
      " |-- tech_want: string (nullable = true)\n",
      " |-- aliens: string (nullable = true)\n",
      " |-- programming_ability: double (nullable = true)\n",
      " |-- employment_status: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- company_size_range: string (nullable = true)\n",
      " |-- team_size_range: string (nullable = true)\n",
      " |-- women_on_team: string (nullable = true)\n",
      " |-- remote: string (nullable = true)\n",
      " |-- job_satisfaction: string (nullable = true)\n",
      " |-- job_discovery: string (nullable = true)\n",
      " |-- dev_environment: string (nullable = true)\n",
      " |-- commit_frequency: string (nullable = true)\n",
      " |-- hobby: string (nullable = true)\n",
      " |-- dogs_vs_cats: string (nullable = true)\n",
      " |-- desktop_os: string (nullable = true)\n",
      " |-- unit_testing: string (nullable = true)\n",
      " |-- rep_range: string (nullable = true)\n",
      " |-- visit_frequency: string (nullable = true)\n",
      " |-- why_learn_new_tech: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- open_to_new_job: string (nullable = true)\n",
      " |-- new_job_value: string (nullable = true)\n",
      " |-- job_search_annoyance: string (nullable = true)\n",
      " |-- interview_likelihood: string (nullable = true)\n",
      " |-- how_to_improve_interview_process: string (nullable = true)\n",
      " |-- star_wars_vs_star_trek: string (nullable = true)\n",
      " |-- agree_tech: string (nullable = true)\n",
      " |-- agree_notice: string (nullable = true)\n",
      " |-- agree_problemsolving: string (nullable = true)\n",
      " |-- agree_diversity: string (nullable = true)\n",
      " |-- agree_adblocker: string (nullable = true)\n",
      " |-- agree_alcohol: string (nullable = true)\n",
      " |-- agree_loveboss: string (nullable = true)\n",
      " |-- agree_nightcode: string (nullable = true)\n",
      " |-- agree_legacy: string (nullable = true)\n",
      " |-- agree_mars: string (nullable = true)\n",
      " |-- important_variety: string (nullable = true)\n",
      " |-- important_control: string (nullable = true)\n",
      " |-- important_sameend: string (nullable = true)\n",
      " |-- important_newtech: string (nullable = true)\n",
      " |-- important_buildnew: string (nullable = true)\n",
      " |-- important_buildexisting: string (nullable = true)\n",
      " |-- important_promotion: string (nullable = true)\n",
      " |-- important_companymission: string (nullable = true)\n",
      " |-- important_wfh: string (nullable = true)\n",
      " |-- important_ownoffice: string (nullable = true)\n",
      " |-- developer_challenges: string (nullable = true)\n",
      " |-- why_stack_overflow: string (nullable = true)\n",
      "\n",
      "=== Print the selected columns of the table ===\n",
      "+-----------+--------------------+------------+---------------+\n",
      "|    country|          occupation|age_midpoint|salary_midpoint|\n",
      "+-----------+--------------------+------------+---------------+\n",
      "|Afghanistan|                null|        22.0|           null|\n",
      "|Afghanistan|Mobile developer ...|        32.0|        45000.0|\n",
      "|Afghanistan|                null|        null|           null|\n",
      "|Afghanistan|              DevOps|        null|         5000.0|\n",
      "|Afghanistan|                null|        65.0|           null|\n",
      "|Afghanistan|                null|        22.0|           null|\n",
      "|Afghanistan|       Growth hacker|        null|       210000.0|\n",
      "|Afghanistan|Back-end web deve...|        27.0|         5000.0|\n",
      "|    Albania|                null|        27.0|           null|\n",
      "|    Albania|Back-end web deve...|        22.0|         5000.0|\n",
      "|    Albania|Full-stack web de...|        27.0|         5000.0|\n",
      "|    Albania|Full-stack web de...|        22.0|        15000.0|\n",
      "|    Albania|Full-stack web de...|        27.0|         5000.0|\n",
      "|    Albania|Back-end web deve...|        27.0|         5000.0|\n",
      "|    Albania|Back-end web deve...|        22.0|        15000.0|\n",
      "|    Algeria|                null|        44.5|           null|\n",
      "|    Algeria|   Desktop developer|        27.0|           null|\n",
      "|    Algeria|             Student|        16.0|           null|\n",
      "|    Algeria|                null|        22.0|           null|\n",
      "|    Algeria|   Desktop developer|        27.0|        15000.0|\n",
      "+-----------+--------------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Print records where the response is from Afghanistan ===\n",
      "+-----------+--------------------+------------+---------------+\n",
      "|    country|          occupation|age_midpoint|salary_midpoint|\n",
      "+-----------+--------------------+------------+---------------+\n",
      "|Afghanistan|                null|        22.0|           null|\n",
      "|Afghanistan|Mobile developer ...|        32.0|        45000.0|\n",
      "|Afghanistan|                null|        null|           null|\n",
      "|Afghanistan|              DevOps|        null|         5000.0|\n",
      "|Afghanistan|                null|        65.0|           null|\n",
      "|Afghanistan|                null|        22.0|           null|\n",
      "|Afghanistan|       Growth hacker|        null|       210000.0|\n",
      "|Afghanistan|Back-end web deve...|        27.0|         5000.0|\n",
      "+-----------+--------------------+------------+---------------+\n",
      "\n",
      "=== Print the count of occupations ===\n",
      "+--------------------+-----+\n",
      "|          occupation|count|\n",
      "+--------------------+-----+\n",
      "|     Product manager|   18|\n",
      "|Business intellig...|    8|\n",
      "|Mobile developer ...|    3|\n",
      "|System administrator|   34|\n",
      "|             Student|  234|\n",
      "|    Mobile developer|   60|\n",
      "| Engineering manager|   22|\n",
      "|                null|  297|\n",
      "|            Designer|   15|\n",
      "|Embedded applicat...|   33|\n",
      "| Graphics programmer|   11|\n",
      "|               other|   67|\n",
      "|   Desktop developer|   87|\n",
      "|Developer with a ...|   25|\n",
      "|       Growth hacker|    5|\n",
      "|   Quality Assurance|   10|\n",
      "|             Analyst|   20|\n",
      "|Full-stack web de...|  498|\n",
      "|Mobile developer ...|   32|\n",
      "|Machine learning ...|   10|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Print records with average mid age less than 20 ===\n",
      "+---------+--------------------+------------+---------------+\n",
      "|  country|          occupation|age_midpoint|salary_midpoint|\n",
      "+---------+--------------------+------------+---------------+\n",
      "|  Algeria|             Student|        16.0|           null|\n",
      "|  Algeria|Back-end web deve...|        16.0|           null|\n",
      "|Argentina|             Student|        16.0|         5000.0|\n",
      "|Argentina|Back-end web deve...|        16.0|         5000.0|\n",
      "|  Armenia|Back-end web deve...|        16.0|         5000.0|\n",
      "|  Armenia|                null|        16.0|           null|\n",
      "|  Armenia|Mobile developer ...|        16.0|         5000.0|\n",
      "|  Armenia|Mobile developer ...|        16.0|         5000.0|\n",
      "|  Austria|Mobile developer ...|        16.0|           null|\n",
      "|  Austria|Full-stack web de...|        16.0|           null|\n",
      "|  Austria|Full-stack web de...|        16.0|        15000.0|\n",
      "|  Austria|                null|        16.0|           null|\n",
      "|  Austria|             Student|        16.0|           null|\n",
      "|  Austria|                null|        16.0|           null|\n",
      "|  Austria|             Student|        16.0|           null|\n",
      "|  Austria|             Student|        16.0|           null|\n",
      "|  Austria|Back-end web deve...|        16.0|         5000.0|\n",
      "|  Austria|                null|        16.0|         5000.0|\n",
      "|  Austria|                null|        16.0|           null|\n",
      "|  Austria|Back-end web deve...|        16.0|         5000.0|\n",
      "+---------+--------------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Print the result by salary middle point in descending order ===\n",
      "+------------------+--------------------+------------+---------------+\n",
      "|           country|          occupation|age_midpoint|salary_midpoint|\n",
      "+------------------+--------------------+------------+---------------+\n",
      "|         Argentina|Back-end web deve...|        32.0|       210000.0|\n",
      "|           Denmark|              DevOps|        44.5|       210000.0|\n",
      "|         Argentina|Full-stack web de...|        27.0|       210000.0|\n",
      "|           Denmark|Enterprise level ...|        32.0|       210000.0|\n",
      "|Dominican Republic|Executive (VP of ...|        37.0|       210000.0|\n",
      "|             China|Machine learning ...|        22.0|       210000.0|\n",
      "|            France|Full-stack web de...|        32.0|       210000.0|\n",
      "|           Denmark|Full-stack web de...|        22.0|       210000.0|\n",
      "|       Afghanistan|       Growth hacker|        null|       210000.0|\n",
      "|          Bulgaria|Enterprise level ...|        37.0|       195000.0|\n",
      "|           Denmark|Full-stack web de...|        32.0|       185000.0|\n",
      "|           Austria|               other|        37.0|       185000.0|\n",
      "|           Belgium|Database administ...|        37.0|       165000.0|\n",
      "|         Australia|   Desktop developer|        65.0|       165000.0|\n",
      "|           Denmark|Business intellig...|        44.5|       165000.0|\n",
      "|            France|Enterprise level ...|        54.5|       165000.0|\n",
      "|           Germany|Executive (VP of ...|        27.0|       165000.0|\n",
      "|            Canada|Mobile developer ...|        32.0|       155000.0|\n",
      "|            Brazil|              DevOps|        32.0|       155000.0|\n",
      "|           Denmark|Developer with a ...|        32.0|       155000.0|\n",
      "+------------------+--------------------+------------+---------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Group by country and aggregate by average salary middle point ===\n",
      "+------------------+--------------------+\n",
      "|           country|avg(salary_midpoint)|\n",
      "+------------------+--------------------+\n",
      "|           Germany|  46491.228070175435|\n",
      "|       Afghanistan|             66250.0|\n",
      "|          Cambodia|              5000.0|\n",
      "|            France|  39648.760330578516|\n",
      "|           Algeria|             30000.0|\n",
      "|         Argentina|  27950.819672131147|\n",
      "|           Belgium|   45989.01098901099|\n",
      "|           Ecuador|             40000.0|\n",
      "|           Albania|   8333.333333333334|\n",
      "|           Finland|   45714.28571428572|\n",
      "|           Bahamas|             95000.0|\n",
      "|             China|             54687.5|\n",
      "|           Belarus|             10000.0|\n",
      "|             Chile|  41666.666666666664|\n",
      "|           Croatia|  14166.666666666666|\n",
      "|           Andorra|             40000.0|\n",
      "|           Bolivia|              5000.0|\n",
      "|           Denmark|   68768.65671641791|\n",
      "|        Bangladesh|   7307.692307692308|\n",
      "|Bosnia Herzegovina|             10000.0|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== With salary bucket column ===\n",
      "+---------------+----------------------+\n",
      "|salary_midpoint|salary_midpoint_bucket|\n",
      "+---------------+----------------------+\n",
      "|           null|                  null|\n",
      "|        45000.0|                 40000|\n",
      "|           null|                  null|\n",
      "|         5000.0|                     0|\n",
      "|           null|                  null|\n",
      "|           null|                  null|\n",
      "|       210000.0|                200000|\n",
      "|         5000.0|                     0|\n",
      "|           null|                  null|\n",
      "|         5000.0|                     0|\n",
      "|         5000.0|                     0|\n",
      "|        15000.0|                     0|\n",
      "|         5000.0|                     0|\n",
      "|         5000.0|                     0|\n",
      "|        15000.0|                     0|\n",
      "|           null|                  null|\n",
      "|           null|                  null|\n",
      "|           null|                  null|\n",
      "|           null|                  null|\n",
      "|        15000.0|                     0|\n",
      "+---------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Group by salary bucket ===\n",
      "+----------------------+-----+\n",
      "|salary_midpoint_bucket|count|\n",
      "+----------------------+-----+\n",
      "|                  null|  566|\n",
      "|                     0|  523|\n",
      "|                 20000|  351|\n",
      "|                 40000|  260|\n",
      "|                 60000|  134|\n",
      "|                 80000|   63|\n",
      "|                100000|   51|\n",
      "|                120000|   23|\n",
      "|                140000|   11|\n",
      "|                160000|    5|\n",
      "|                180000|    3|\n",
      "|                200000|    9|\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "AGE_MIDPOINT: String = age_midpoint\r\n",
       "SALARY_MIDPOINT: String = salary_midpoint\r\n",
       "SALARY_MIDPOINT_BUCKET: String = salary_midpoint_bucket\r\n",
       "session: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@66056a28\r\n",
       "dataFrameReader: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@3184e4c\r\n",
       "responses: org.apache.spark.sql.DataFrame = [_c0: int, collector: string ... 64 more fields]\r\n",
       "responseWithSelectedColumns: org.apache.spark.sql.DataFrame = [country: string, occupation: string ... 2 more fields]\r\n",
       "groupedDataset: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [occupation: string], value: [country: string, occupation: string ... 2 more fields], type: GroupBy]\r",
       "..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val AGE_MIDPOINT = \"age_midpoint\"\n",
    "val SALARY_MIDPOINT = \"salary_midpoint\"\n",
    "val SALARY_MIDPOINT_BUCKET = \"salary_midpoint_bucket\"\n",
    "\n",
    "val session = SparkSession.builder().appName(\"StackOverFlowSurvey\").master(\"local[1]\").getOrCreate()\n",
    "\n",
    "val dataFrameReader = session.read\n",
    "\n",
    "val responses = dataFrameReader\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", value = true)\n",
    "  .csv(\"input/2016-stack-overflow-survey-responses.csv\")\n",
    "\n",
    "System.out.println(\"=== Print out schema ===\")\n",
    "responses.printSchema()\n",
    "\n",
    "val responseWithSelectedColumns = responses.select(\"country\", \"occupation\", AGE_MIDPOINT, SALARY_MIDPOINT)\n",
    "\n",
    "System.out.println(\"=== Print the selected columns of the table ===\")\n",
    "responseWithSelectedColumns.show()\n",
    "\n",
    "System.out.println(\"=== Print records where the response is from Afghanistan ===\")\n",
    "responseWithSelectedColumns.filter(responseWithSelectedColumns.col(\"country\").===(\"Afghanistan\")).show()\n",
    "\n",
    "System.out.println(\"=== Print the count of occupations ===\")\n",
    "val groupedDataset = responseWithSelectedColumns.groupBy(\"occupation\")\n",
    "groupedDataset.count().show()\n",
    "\n",
    "System.out.println(\"=== Print records with average mid age less than 20 ===\")\n",
    "responseWithSelectedColumns.filter(responseWithSelectedColumns.col(AGE_MIDPOINT) < 20).show()\n",
    "\n",
    "System.out.println(\"=== Print the result by salary middle point in descending order ===\")\n",
    "responseWithSelectedColumns.orderBy(responseWithSelectedColumns.col(SALARY_MIDPOINT).desc).show()\n",
    "\n",
    "System.out.println(\"=== Group by country and aggregate by average salary middle point ===\")\n",
    "val datasetGroupByCountry = responseWithSelectedColumns.groupBy(\"country\")\n",
    "datasetGroupByCountry.avg(SALARY_MIDPOINT).show()\n",
    "\n",
    "val responseWithSalaryBucket = responses.withColumn(SALARY_MIDPOINT_BUCKET,\n",
    "  responses.col(SALARY_MIDPOINT).divide(20000).cast(\"integer\").multiply(20000))\n",
    "\n",
    "System.out.println(\"=== With salary bucket column ===\")\n",
    "responseWithSalaryBucket.select(SALARY_MIDPOINT, SALARY_MIDPOINT_BUCKET).show()\n",
    "\n",
    "System.out.println(\"=== Group by salary bucket ===\")\n",
    "responseWithSalaryBucket.groupBy(SALARY_MIDPOINT_BUCKET).count().orderBy(SALARY_MIDPOINT_BUCKET).show()\n",
    "\n",
    "session.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un programa Spark para leer los datos de la casa desde / RealEstate.csv, agrupe por ubicación, agregue el precio promedio por SQ Ft y ordene por precio promedio por SQ Ft.\n",
    "\n",
    "El conjunto de datos de casas contiene una colección de listados de bienes inmuebles recientes en el condado de San Luis Obispo y\n",
    "alrededor.\n",
    "\n",
    "El conjunto de datos contiene los siguientes campos:\n",
    "1. MLS: número de servicio de listado múltiple para la casa (identificación única).\n",
    "2. Ubicación: ciudad / pueblo donde se encuentra la casa. La mayoría de las ubicaciones están en el condado de San Luis Obispo y norte del condado de Santa Bárbara (Santa Maria Orcutt, Lompoc, Guadalupe, Los Alamos), pero allí algunas ubicaciones fuera del área también.\n",
    "3. Precio: el precio de listado más reciente de la casa (en dólares).\n",
    "4. Dormitorios: número de dormitorios.\n",
    "5. Baños: número de baños.\n",
    "6. Tamaño: tamaño de la casa en pies cuadrados.\n",
    "7. Precio / Pies Cuadrados: precio de la casa por pie cuadrado.\n",
    "8. Estado: tipo de venta. Estos tipos están representados en el conjunto de datos: venta corta, ejecución hipotecaria y regular.\n",
    "\n",
    "Cada campo está separado por comas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|           Location|  avg(Price SQ Ft)|\n",
      "+-------------------+------------------+\n",
      "|         New Cuyama|             34.05|\n",
      "|        Bakersfield|             69.69|\n",
      "|          King City| 71.51333333333334|\n",
      "|         Greenfield|             91.58|\n",
      "|    Santa Margarita|             95.38|\n",
      "|            Soledad|102.69333333333333|\n",
      "|        Out Of Area|116.23333333333333|\n",
      "|          Guadalupe|           120.175|\n",
      "|           Coalinga|124.34285714285714|\n",
      "| Santa Maria-Orcutt|147.58871698113194|\n",
      "|             Lompoc|             149.9|\n",
      "|             Lompoc|159.87115384615387|\n",
      "|         San Miguel|163.16071428571425|\n",
      "|            Bradley|            166.81|\n",
      "|            Creston|            181.76|\n",
      "| Santa Maria-Orcutt|183.03692307692307|\n",
      "|             Nipomo|187.92333333333332|\n",
      "|        Paso Robles|191.17752941176474|\n",
      "|         Los Alamos|191.99333333333334|\n",
      "|            Solvang|           193.305|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "PRICE_SQ_FT: String = Price SQ Ft\r\n",
       "session: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3a5cea33\r\n",
       "realEstate: org.apache.spark.sql.DataFrame = [MLS: int, Location: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val PRICE_SQ_FT = \"Price SQ Ft\"\n",
    "\n",
    "val session = SparkSession.builder().appName(\"HousePriceSolution\").master(\"local[1]\").getOrCreate()\n",
    "\n",
    "val realEstate = session.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", value = true)\n",
    "      .csv(\"input/RealEstate.csv\")\n",
    "\n",
    "realEstate.groupBy(\"Location\")\n",
    "      .avg(PRICE_SQ_FT)\n",
    "      .orderBy(\"avg(Price SQ Ft)\")\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Join VS Spark Core Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark SQL admite los mismos tipos basicos de union que Spark core.\n",
    "- El optimizador de Spark SQL Catalyst puede hacer mas trabajo pesado por nosotros para optimizar  el rendimiento de la union.\n",
    "- Al usar Spark SQL join, disminuira nuestras capacidades de control. Por ejemplo: Spark SQL puede postergar o reordenar operaciones para hacer que las uniones sean mas eficientes. La desventaja es que no tenemos contros sobre el particionador de conjuntos de datos por lo que no podemos evitar manualmente el reordenamiento de la informacion tal como hicimos con las uniones Spark Core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de Spark SQL Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los tipos de uniones SQL estandar son compatibles con Spark SQL y pueden ser especificados como **JoinType** cuando realizamos una union\n",
    "- Tipos de uniones:\n",
    "    1. Inner\n",
    "    2. Outer\n",
    "    3. Left Outer\n",
    "    4. Right outer\n",
    "    5. Left Semi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Print 20 records of makerspace table ===\n",
      "+--------------------+--------+\n",
      "|  Name of makerspace|Postcode|\n",
      "+--------------------+--------+\n",
      "|        Hub Workshop|SE15 3SN|\n",
      "|Nottingham Hacksp...| NG3 1JH|\n",
      "|         Farset Labs|BT12 5GH|\n",
      "|       Medway Makers| ME4 3JE|\n",
      "|             fizzPop|  B5 5SR|\n",
      "|South London Make...|SE24 9AA|\n",
      "|Create Space London | HA9 6DE|\n",
      "|          FounderHub|CF10 1DY|\n",
      "|  LuneLab Makerspace| LA2 6ND|\n",
      "|            The Shed| CT2 7NF|\n",
      "|      Build Brighton| BN2 4AB|\n",
      "|           Makespace| CB2 1RX|\n",
      "|   Swansea Hackspace| SA1 1DP|\n",
      "|57North (previous...|AB11 5BN|\n",
      "|        BEC Fab Lab |CA13 0HT|\n",
      "|   Dundee MakerSpace| DD1 4QB|\n",
      "|                EPIK| CT3 4GP|\n",
      "|Fab Lab Nerve Centre|BT48 6HJ|\n",
      "|  fablab@strathclyde|  G1 1XJ|\n",
      "|MakerspaceFY1 (Bl...| FY1 4DY|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Print 20 records of postcode table ===\n",
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|PostCode|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|       Region|Postcodes|Active postcodes|Population|Households|\n",
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|    AB1 | 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|     Aberdeen|     2655|               0|      null|      null|\n",
      "|    AB2 | 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|     Aberdeen|     3070|               0|      null|      null|\n",
      "|    AB3 | 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|     Aberdeen|     2168|               0|      null|      null|\n",
      "|    AB4 | 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|     Aberdeen|     2956|               0|      null|      null|\n",
      "|    AB5 | 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|     Aberdeen|     3002|               0|      null|      null|\n",
      "|    AB9 | 57.1466|  -2.1142| 393189|  806196|NJ931061|            Aberdeen|     Aberdeen|     1066|               0|      null|      null|\n",
      "|   AB10 | 57.1348| -2.11748| 392988|  804882|NJ929048|Aberdeen city cen...|     Aberdeen|      888|             675|     21964|     11517|\n",
      "|   AB11 | 57.1371| -2.09341| 394445|  805136|NJ944051|Aberdeen city cen...|     Aberdeen|      889|             644|     21237|     10926|\n",
      "|   AB12 | 57.1033| -2.11034| 393414|  801375|NJ934013|Aberdeen, Altens,...|     Aberdeen|      991|             782|     25414|     10688|\n",
      "|   AB13 | 57.1127| -2.24469| 385279|  802443|NJ852024|          Milltimber|     Aberdeen|      100|              80|      2725|       947|\n",
      "|   AB14 | 57.1033| -2.27251| 383590|  801402|NJ835014|Peterculter, Uppe...|     Aberdeen|      164|             140|      4881|      2162|\n",
      "|   AB15 | 57.1388| -2.16551| 390082|  805334|NJ900053|Aberdeen, Bieldsi...|     Aberdeen|     1205|            1019|     35543|     15330|\n",
      "|   AB16 | 57.1596| -2.15654| 390630|  807648|NJ906076|Aberdeen, Mastric...|     Aberdeen|      893|             776|     29238|     12874|\n",
      "|   AB21 | 57.2091| -2.20174| 387912|  813165|NJ879131|Aberdeen, Blackbu...|     Aberdeen|      879|             732|     22181|      9650|\n",
      "|   AB22 | 57.1864| -2.11913| 392898|  810627|NJ928106|Aberdeen, Bridge ...|     Aberdeen|      361|             300|     16311|      6978|\n",
      "|   AB23 | 57.2088| -2.08971| 394680|  813118|NJ946131|Aberdeen, Balmedi...|     Aberdeen|      399|             311|     11143|      4517|\n",
      "|   AB24 | 57.1634| -2.10828| 393550|  808065|NJ935080|Aberdeen, Old Abe...|     Aberdeen|      968|             808|     36343|     16935|\n",
      "|   AB25 | 57.1534| -2.11422| 393189|  806953|NJ931069|Aberdeen city cen...|     Aberdeen|      610|             476|     18407|      9634|\n",
      "|   AB30 |  56.846| -2.47726| 370986|  772829|NO709728|        Laurencekirk|Aberdeenshire|      349|             308|      7229|      2935|\n",
      "|   AB31 | 57.0672| -2.50552| 369444|  797465|NO694974|            Banchory|Aberdeenshire|     1033|             629|     15319|      6096|\n",
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Group by Region ===\n",
      "+--------------------+-----+\n",
      "|              Region|count|\n",
      "+--------------------+-----+\n",
      "|       Staffordshire|    1|\n",
      "|    Scottish Borders|    1|\n",
      "|          Sunderland|    1|\n",
      "|          Manchester|    3|\n",
      "|            Bradford|    1|\n",
      "|          Wandsworth|    1|\n",
      "|            Coventry|    1|\n",
      "|             Lambeth|    2|\n",
      "|              Oxford|    2|\n",
      "|          Eastbourne|    1|\n",
      "|Cheshire West and...|    1|\n",
      "|             Glasgow|    3|\n",
      "|             Swindon|    1|\n",
      "|           Cambridge|    1|\n",
      "|             Belfast|    2|\n",
      "|             Cardiff|    3|\n",
      "|              Camden|    2|\n",
      "|              Dundee|    1|\n",
      "|                null|    2|\n",
      "|              Dudley|    1|\n",
      "|               Moray|    1|\n",
      "|          Birmingham|    1|\n",
      "|       Tower Hamlets|    3|\n",
      "|              Exeter|    1|\n",
      "|           Leicester|    1|\n",
      "|               Derby|    1|\n",
      "|   Barrow-in-Furness|    1|\n",
      "|Kensington and Ch...|    1|\n",
      "|            Plymouth|    1|\n",
      "|               Brent|    1|\n",
      "| North Hertfordshire|    1|\n",
      "|         Argyllshire|    1|\n",
      "|           Sheffield|    2|\n",
      "|              Medway|    1|\n",
      "|           Lancaster|    1|\n",
      "|             Falkirk|    1|\n",
      "|             Newport|    1|\n",
      "|          Tewkesbury|    1|\n",
      "|         Westminster|    1|\n",
      "|             Lincoln|    1|\n",
      "|             Hackney|    1|\n",
      "|           Southwark|    2|\n",
      "|            Aberdeen|    2|\n",
      "|          Colchester|    1|\n",
      "|          Canterbury|    1|\n",
      "|           Allerdale|    1|\n",
      "|             Bristol|    2|\n",
      "|                Kent|    1|\n",
      "|         Southampton|    1|\n",
      "|             Enfield|    1|\n",
      "|             Swansea|    1|\n",
      "|            Hereford|    1|\n",
      "|       Milton Keynes|    1|\n",
      "|            Rushmoor|    1|\n",
      "|             Fenland|    1|\n",
      "|              Ealing|    1|\n",
      "|           Liverpool|    2|\n",
      "|   Brighton and Hove|    2|\n",
      "|             Gwynedd|    1|\n",
      "|            Cornwall|    1|\n",
      "|      Waltham Forest|    1|\n",
      "|           Newcastle|    1|\n",
      "|             Gedling|    1|\n",
      "|   Rhondda Cynon Taf|    1|\n",
      "|Richmond upon Thames|    1|\n",
      "|             Wealden|    1|\n",
      "|               Leeds|    2|\n",
      "|           Blackpool|    1|\n",
      "|      City of London|    1|\n",
      "|               Wigan|    1|\n",
      "|                York|    1|\n",
      "|            Lewisham|    1|\n",
      "|               Derry|    1|\n",
      "|           Berkshire|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SparkSession, functions}\r\n",
       "session: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7302274c\r\n",
       "makerSpace: org.apache.spark.sql.DataFrame = [Timestamp: string, Collected by: string ... 37 more fields]\r\n",
       "postCode: org.apache.spark.sql.DataFrame = [PostCode: string, Latitude: string ... 10 more fields]\r\n",
       "joined: org.apache.spark.sql.DataFrame = [Timestamp: string, Collected by: string ... 49 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession, functions}\n",
    "// Declaramos el SparkSession que es similar a la funcion que cumple SparkContext\n",
    "val session = SparkSession.builder().appName(\"UkMakerSpaces\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "val makerSpace = session.read.option(\"header\", \"true\").csv(\"input/uk-makerspaces-identifiable-data.csv\")\n",
    "\n",
    "val postCode = session.read.option(\"header\", \"true\").csv(\"input/uk-postcode.csv\")\n",
    "   .withColumn(\"PostCode\", functions.concat_ws(\"\", functions.col(\"PostCode\"), functions.lit(\" \")))\n",
    "\n",
    "System.out.println(\"=== Print 20 records of makerspace table ===\")\n",
    "makerSpace.select(\"Name of makerspace\", \"Postcode\").show()\n",
    "\n",
    "System.out.println(\"=== Print 20 records of postcode table ===\")\n",
    "postCode.show()\n",
    "\n",
    "val joined = makerSpace.join(postCode, makerSpace.col(\"Postcode\").startsWith(postCode.col(\"Postcode\")), \"left_outer\")\n",
    "\n",
    "System.out.println(\"=== Group by Region ===\")\n",
    "joined.groupBy(\"Region\").count().show(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjuntos de datos Fuertemente tipados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Un conjunto de datos es una coleccion de datos que tiene una estructura y que se pueden tranasformar en paralelo mediante operaciones funcionales o relazacionales.\n",
    "- Cada conjunto de datos tamibien tiene una vista orgzanizada en columnas llamada **Dataframe**, que es un conjunto de datos de Objetos tipo fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Print out schema ===\n",
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- age_midpoint: double (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- salary_midpoint: double (nullable = true)\n",
      "\n",
      "=== Print 20 records of responses table ===\n",
      "+-----------+------------+--------------------+---------------+\n",
      "|    country|age_midpoint|          occupation|salary_midpoint|\n",
      "+-----------+------------+--------------------+---------------+\n",
      "|Afghanistan|        22.0|                null|           null|\n",
      "|Afghanistan|        32.0|Mobile developer ...|        45000.0|\n",
      "|Afghanistan|        null|                null|           null|\n",
      "|Afghanistan|        null|              DevOps|         5000.0|\n",
      "|Afghanistan|        65.0|                null|           null|\n",
      "|Afghanistan|        22.0|                null|           null|\n",
      "|Afghanistan|        null|       Growth hacker|       210000.0|\n",
      "|Afghanistan|        27.0|Back-end web deve...|         5000.0|\n",
      "|    Albania|        27.0|                null|           null|\n",
      "|    Albania|        22.0|Back-end web deve...|         5000.0|\n",
      "|    Albania|        27.0|Full-stack web de...|         5000.0|\n",
      "|    Albania|        22.0|Full-stack web de...|        15000.0|\n",
      "|    Albania|        27.0|Full-stack web de...|         5000.0|\n",
      "|    Albania|        27.0|Back-end web deve...|         5000.0|\n",
      "|    Albania|        22.0|Back-end web deve...|        15000.0|\n",
      "|    Algeria|        44.5|                null|           null|\n",
      "|    Algeria|        27.0|   Desktop developer|           null|\n",
      "|    Algeria|        16.0|             Student|           null|\n",
      "|    Algeria|        22.0|                null|           null|\n",
      "|    Algeria|        27.0|   Desktop developer|        15000.0|\n",
      "+-----------+------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Print the responses from Afghanistan ===\n"
     ]
    },
    {
     "ename": "java.lang.NullPointerException\r",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.lang.NullPointerException\r",
      "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "  at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r",
      "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r",
      "  at java.lang.reflect.Method.invoke(Unknown Source)\r",
      "  at org.apache.spark.sql.catalyst.encoders.OuterScopes$$anonfun$getOuterScope$1.apply(OuterScopes.scala:70)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance$$anonfun$10.apply(objects.scala:485)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance$$anonfun$10.apply(objects.scala:485)\r",
      "  at scala.Option.map(Option.scala:146)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.doGenCode(objects.scala:485)\r",
      "  at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\r",
      "  at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\r",
      "  at scala.Option.getOrElse(Option.scala:121)\r",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike$$anonfun$3.apply(objects.scala:99)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike$$anonfun$3.apply(objects.scala:98)\r",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\r",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r",
      "  at scala.collection.immutable.List.map(List.scala:296)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.InvokeLike$class.prepareArguments(objects.scala:98)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.Invoke.prepareArguments(objects.scala:303)\r",
      "  at org.apache.spark.sql.catalyst.expressions.objects.Invoke.doGenCode(objects.scala:348)\r",
      "  at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\r",
      "  at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\r",
      "  at scala.Option.getOrElse(Option.scala:121)\r",
      "  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\r",
      "  at org.apache.spark.sql.execution.FilterExec.org$apache$spark$sql$execution$FilterExec$$genPredicate$1(basicPhysicalOperators.scala:141)\r",
      "  at org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:185)\r",
      "  at org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:166)\r",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\r",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r",
      "  at scala.collection.immutable.List.map(List.scala:296)\r",
      "  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:166)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\r",
      "  at org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\r",
      "  at org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:172)\r",
      "  at org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:85)\r",
      "  at org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\r",
      "  at org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\r",
      "  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:127)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\r",
      "  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:87)\r",
      "  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r",
      "  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\r",
      "  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\r",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)\r",
      "  ... 31 elided",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val AGE_MIDPOINT = \"age_midpoint\"\n",
    "val SALARY_MIDPOINT = \"salary_midpoint\"\n",
    "val SALARY_MIDPOINT_BUCKET = \"salaryMidpointBucket\"\n",
    "\n",
    "val session = SparkSession.builder().appName(\"StackOverFlowSurvey\").master(\"local[*]\").getOrCreate()\n",
    "val dataFrameReader = session.read\n",
    "\n",
    "val responses = dataFrameReader\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", value = true)\n",
    "  .csv(\"input/2016-stack-overflow-survey-responses.csv\")\n",
    "\n",
    "val responseWithSelectedColumns = responses.select(\"country\", \"age_midpoint\", \"occupation\", \"salary_midpoint\")\n",
    "\n",
    "case class Response(country: String, age_midpoint: Option[Double], occupation: String, salary_midpoint: Option[Double])\n",
    "\n",
    "import session.implicits._\n",
    "val typedDataset = responseWithSelectedColumns.as[Response]\n",
    "\n",
    "System.out.println(\"=== Print out schema ===\")\n",
    "typedDataset.printSchema()\n",
    "\n",
    "System.out.println(\"=== Print 20 records of responses table ===\")\n",
    "typedDataset.show(20)\n",
    "\n",
    "System.out.println(\"=== Print the responses from Afghanistan ===\")\n",
    "typedDataset.filter(response => response.country == \"Afghanistan\").show()\n",
    "\n",
    "System.out.println(\"=== Print the count of occupations ===\")\n",
    "typedDataset.groupBy(typedDataset.col(\"occupation\")).count().show()\n",
    "\n",
    "System.out.println(\"=== Print responses with average mid age less than 20 ===\")\n",
    "typedDataset.filter(response => response.age_midpoint.isDefined && response.age_midpoint.get < 20.0).show()\n",
    "\n",
    "System.out.println(\"=== Print the result by salary middle point in descending order ===\")\n",
    "typedDataset.orderBy(typedDataset.col(SALARY_MIDPOINT).desc).show()\n",
    "\n",
    "System.out.println(\"=== Group by country and aggregate by average salary middle point ===\")\n",
    "typedDataset.filter(response => response.salary_midpoint.isDefined).groupBy(\"country\").avg(SALARY_MIDPOINT).show()\n",
    "\n",
    "System.out.println(\"=== Group by salary bucket ===\")\n",
    "typedDataset.map(response => response.salary_midpoint.map(point => Math.round(point / 20000) * 20000).orElse(None))\n",
    "  .withColumnRenamed(\"value\", SALARY_MIDPOINT_BUCKET)\n",
    "  .groupBy(SALARY_MIDPOINT_BUCKET)\n",
    "  .count()\n",
    "  .orderBy(SALARY_MIDPOINT_BUCKET).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\codificadores.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un Dataframe desde un Archivo Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassNotFoundException",
     "evalue": " Failed to find data source: com.crealytics.spark.excel. Please find packages at http://spark.apache.org/third-party-projects.html\r",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassNotFoundException: Failed to find data source: com.crealytics.spark.excel. Please find packages at http://spark.apache.org/third-party-projects.html\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\r",
      "  ... 36 elided\r",
      "Caused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r",
      "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)\r",
      "  at java.lang.ClassLoader.loadClass(Unknown Source)\r",
      "  at java.lang.ClassLoader.loadClass(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r",
      "  at scala.util.Try$.apply(Try.scala:192)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r",
      "  at scala.util.Try.orElse(Try.scala:84)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\r",
      "  ... 38 more",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "//val session = SparkSession.builder().appName(\"StackOverFlowSurvey\").master(\"local[1]\").getOrCreate()\n",
    "val dataFrameReader = session.read\n",
    "\n",
    "val spark: SparkSession = SparkSession.builder().appName(\"StackOverFlowSurvey\").master(\"local[1]\").getOrCreate()\n",
    "//val dataFrameReader = session.read\n",
    "\n",
    "val df = spark.read\n",
    "        .format(\"com.crealytics.spark.excel\")\n",
    "        .option(\"sheetName\", \"datos\") // Required\n",
    "        .option(\"useHeader\", \"true\") // Required\n",
    "        .option(\"treatEmptyValuesAsNulls\", \"false\") // Optional, default: true\n",
    "        .option(\"inferSchema\", \"false\") // Optional, default: false\n",
    "        .option(\"addColorColumns\", \"true\") // Optional, default: false\n",
    "        .option(\"startColumn\", 0) // Optional, default: 0\n",
    "        .option(\"endColumn\", 99) // Optional, default: Int.MaxValue\n",
    "        .option(\"timestampFormat\", \"MM-dd-yyyy HH:mm:ss\") // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]\n",
    "        .option(\"maxRowsInMemory\", 20) // Optional, default None. If set, uses a streaming reader which can help with big files\n",
    "        .option(\"excerptSize\", 10) // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from\n",
    "        .load()\n",
    "\n",
    "\n",
    "\n",
    "//val data = readExcel(\"input/Casos_positivos_de_COVID-19_en_Colombia.xlsx\")\n",
    "\n",
    "//data.show(false)\n",
    "\n",
    "\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "30: error: not found: value sqlContext\r",
     "output_type": "error",
     "traceback": [
      "<console>:30: error: not found: value sqlContext\r",
      "       def readExcel(file: String): DataFrame = sqlContext.read\r",
      "                                                ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "def readExcel(file: String): DataFrame = sqlContext.read\n",
    "    .format(\"com.crealytics.spark.excel\")\n",
    "    .option(\"location\", file)\n",
    "    .option(\"useHeader\", \"true\")\n",
    "    .option(\"treatEmptyValuesAsNulls\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"addColorColumns\", \"False\")\n",
    "    .load()\n",
    "\n",
    "val data = readExcel(\"input/Casos_positivos_de_COVID-19_en_Colombia.xlsx\")\n",
    "\n",
    "data.show(false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassNotFoundException",
     "evalue": " Failed to find data source: com.crealytics.spark.excel. Please find packages at http://spark.apache.org/third-party-projects.html\r",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassNotFoundException: Failed to find data source: com.crealytics.spark.excel. Please find packages at http://spark.apache.org/third-party-projects.html\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r",
      "  ... 37 elided\r",
      "Caused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r",
      "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)\r",
      "  at java.lang.ClassLoader.loadClass(Unknown Source)\r",
      "  at java.lang.ClassLoader.loadClass(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r",
      "  at scala.util.Try$.apply(Try.scala:192)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r",
      "  at scala.util.Try.orElse(Try.scala:84)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\r",
      "  ... 39 more",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "\n",
    "val df_excel= spark.read.\n",
    "                   format(\"com.crealytics.spark.excel\").\n",
    "                   option(\"useHeader\", \"true\").\n",
    "                   option(\"treatEmptyValuesAsNulls\", \"false\").\n",
    "                   option(\"inferSchema\", \"false\"). \n",
    "                   option(\"addColorColumns\", \"false\").load(\"input/Casos_positivos_de_COVID-19_en_Colombia.xlsx\")\n",
    "\n",
    "println(df_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
