{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion a RDDs Clave-Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Muchos conuuntos de datos que vemos en ejemplos de la vida real suelen ser pares clave-valor\n",
    "- Ejemplos:\n",
    "Un conjunto de datos que contiene numeros de pasaporte y los nombres de los propietarios.\n",
    "- El patron tipico de este tipo de conjunto de datos es que cada fila es una clave que esta asociada a uno o multiples valores\n",
    "- Un RDD clave-valor es un tipo particular de RDD que puede almacenar pares Clave-Valor\n",
    "- Son bloques utiles de construccion en muchos programas Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como crear RDDs clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crear RDDs clave-valor a partir de una lista de datos clave-valor estructurada llamada tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creamos una tupla\n",
    "val tuple = (\"Juan\", 25)\n",
    "\n",
    "val name = tuple._1\n",
    "val age = tuple._2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir tuplas en RDDs clave-valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ejemplo 1\n",
    "\n",
    "val tuple = List((\"Lily\", 23), (\"Jack\", 29), (\"Mary\", 29), (\"James\", 8))\n",
    "val pairRDD = sc.parallelize(tuple)\n",
    "\n",
    "pairRDD.coalesce(1).saveAsTextFile(\"out/pair_rdd_from_tuple_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ejemplo 2\n",
    "\n",
    "val inputStrings = List(\"Lily 23\", \"Jack 29\", \"Mary 29\", \"James 8\")\n",
    "val regularRDDs = sc.parallelize(inputStrings)\n",
    "\n",
    "val pairRDD = regularRDDs.map(s => (s.split(\" \")(0), s.split(\" \")(1)))\n",
    "pairRDD.coalesce(1).saveAsTextFile(\"out/pair_rdd_from_regular_rdd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones Filter y MapValue en RDDs clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los RDDs clave-valor pueden utilizar todas las transformaciones disponibles para RDDs regulares, por lo tanto admiten las mismas funciones\n",
    "- Dado que los RDDs clave-valor tienen tuplas, tenemos que ejecutar las funciones que operan en tuplas en lugar de elementos individuales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacion Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La transformacion Filter tambien se puede aplicar a un RDD clave-valor\n",
    "- La transformacion Filter se ejecutaa como una funcion y genera un RDD clave-valor formado por aquellos elementos seleccionados que pasan la funcion filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un programa Spark para leer los datos del aeropuerto desde in / airports.text; generar un par RDD con el nombre del aeropuerto como clave y el nombre del país como valor. Luego, elimine todos los aeropuertos que se encuentran en Estados Unidos y envíe el par RDD a out / airports_not_in_usa_pair_rdd.text Cada fila del archivo de entrada contiene las siguientes columnas:  Identificación del aeropuerto, nombre del aeropuerto, ciudad principal a la que sirve el aeropuerto, país donde se encuentra el aeropuerto, Código IATA / FAA, Código ICAO, Latitud, Longitud, Altitud, Zona horaria, DST, Zona horaria en formato Olson"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "       (\"Kamloops\", \"Canada\")\n",
    "       (\"Wewak Intl\", \"Papua New Guinea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportsRDD = sc.textFile(\"input/airports.text\")\n",
    "\n",
    "val airportPairRDD = airportsRDD.map(line => (line.split(\",\")(1),\n",
    "      line.split(\",\")(3)))\n",
    "val airportsNotInUSA = airportPairRDD.filter(keyValue => keyValue._2 != \"\\\"United States\\\"\")\n",
    "\n",
    "airportsNotInUSA.saveAsTextFile(\"out/airports_not_in_usa_pair_rdd.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones map y mapValues\n",
    "\n",
    "- Transformaciones map tambien funciona para RDDs clave-valor. Se puede utilizar para convertir un RDD en otro.\n",
    "- Sin embargo, cuando se trabaja con RDDs clave-valor, solo queremos acceder al valor de nuestro par clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio:\n",
    "\n",
    "\n",
    "Cree un programa Spark para leer los datos del aeropuerto desde / airports.text, genere un par RDD con el nombre del aeropuerto siendo la clave y el nombre del país el valor. Luego convierta el nombre del país a mayúsculas y salida del par RDD a out / airports_uppercase.text Cada fila del archivo de entrada contiene las siguientes columnas: Identificación del aeropuerto, nombre del aeropuerto, ciudad principal a la que sirve el aeropuerto, país donde se encuentra el aeropuerto, código IATA / FAA, Código OACI, latitud, longitud, altitud, zona horaria, horario de verano, zona horaria en formato Olson"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ejemplo Salida:\n",
    "\n",
    "       (\"Kamloops\", \"CANADA\")\n",
    "       (\"Wewak Intl\", \"PAPUA NEW GUINEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airportsRDD = sc.textFile(\"input/airports.text\")\n",
    "\n",
    "val airportPairRDD = airportsRDD.map((line: String) => (line.split(\",\")(1),\n",
    "      line.split(\",\")(3)))\n",
    "\n",
    "val upperCase = airportPairRDD.mapValues(countryName => countryName.toUpperCase)\n",
    "\n",
    "upperCase.saveAsTextFile(\"out/airports_uppercase.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregaciones reduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuando nuestro conjunto de datos se describe en el formato de pares clave-valor, es bastante comun querer agregar estadisticas a traves de todos los elementos con la misma clave\n",
    "- reduceByKey ejecuta varias operaciones reduce paralelas, una para cada clave en el conjunto de datos, donde cada operacion combina valores que tienen la misma clave\n",
    "- Teniendo en cuenta que los conjuntos de entrada podrian tener un gran numero de claves, la operacion reduceByKey no esta implementada como una accion que genera un valor al programa driver. En su lugar, genera un nuevo RDD que consta de clave y el valor reducido para esa clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupByKey\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Un uso comun en el caso de RDDs clave-valor es la agrupacion de datos por claves por ejemplo: Ver juntas todas las transacciones de una cuenta\n",
    "- Si nuestros datos ya fueron introducidos de la forma deseada,  groupByKey agrupara nuestros datos usando la clave de nuestro RDD clave-valor\n",
    "- Digamos que el RDD clave-valor de input tiene claves del tipo K y valores de tipo V, si ejecutamos groupByKey en este RDD, obtendremos un nuevo RDD clave-valor de tipo K e iterable V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio:\n",
    "\n",
    "\n",
    "- Cree un programa Spark para leer los datos del aeropuerto desde / airports.text, muestra la lista de los nombres de los aeropuertos ubicados en cada país. Cada fila del archivo de entrada contiene las siguientes columnas: Identificación del aeropuerto, nombre del aeropuerto, ciudad principal a la que sirve el aeropuerto, país donde se encuentra el aeropuerto, código IATA / FAA, Código OACI, latitud, longitud, altitud, zona horaria, horario de verano, zona horaria en formato Olson"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Salida:\n",
    "\n",
    "       \"Canada\", List(\"Bagotville\", \"Montreal\", \"Coronation\", ...)\n",
    "       \"Norway\" : List(\"Vigra\", \"Andenes\", \"Alta\", \"Bomoen\", \"Bronnoy\",..)\n",
    "       \"Papua New Guinea\",  List(\"Goroka\", \"Madang\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = sc.textFile(\"input/airports.text\")\n",
    "\n",
    "val countryAndAirportNameAndPair = lines.map(airport => (airport.split(\",\")(3),\n",
    "                                                             airport.split(\",\")(1)))\n",
    "val airportsByCountry = countryAndAirportNameAndPair.groupByKey()\n",
    "\n",
    "for ((country, airportName) <- airportsByCountry.collectAsMap()) println(country + \": \" + airportName.toList)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2\n",
    "\n",
    "GroupByKey Vs ReduceByKey\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = List(\"one\", \"two\", \"two\", \"three\", \"three\", \"three\")\n",
    "val wordsPairRdd = sc.parallelize(words).map(word => (word, 1))\n",
    "\n",
    "val wordCountsWithReduceByKey = wordsPairRdd.reduceByKey((x, y) => x + y).collect()\n",
    "println(\"wordCountsWithReduceByKey: \" + wordCountsWithReduceByKey.toList)\n",
    "\n",
    "val wordCountsWithGroupByKey = wordsPairRdd.groupByKey().mapValues(intIterable => intIterable.size).collect()\n",
    "println(\"wordCountsWithGroupByKey: \" + wordCountsWithGroupByKey.toList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ReduceByKey funciona mucho mejor que GroupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformacion sortByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos ordenar un RDD clave-valor siempre y cuando halla un orden definido en la clave\n",
    "- Una vez que hayamos ordenado nuestro RDD clave-valor, cualquier llamado sobre el RDD clave-valor ordenado, ya sea copiar o guardar, nos retornara datos ordenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[8] at parallelize at <console>:25\r\n",
       "sorted1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at sortByKey at <console>:33\r\n",
       "res2: Array[(String, Int)] = Array((english,57), (english,58), (math,55), (math,56), (science,59), (science,54))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq(\n",
    "                            (\"math\",    55),\n",
    "                            (\"math\",    56),\n",
    "                            (\"english\", 57),\n",
    "                            (\"english\", 58),\n",
    "                            (\"science\", 59),\n",
    "                            (\"science\", 54)))\n",
    " rdd.collect()\n",
    "val sorted1 = rdd.sortByKey()\n",
    "sorted1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:28\r\n",
       "rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[7] at sortByKey at <console>:37\r\n",
       "res1: Array[(String, Int)] = Array((Brazil,55), (China,86), (Greece,30), (India,91), (Nepal,977), (Sweden,46), (Turkey,90), (USA,1))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Seq(\n",
    "    (\"India\",91),\n",
    "    (\"USA\",1),\n",
    "    (\"Brazil\",55),\n",
    "    (\"Greece\",30),\n",
    "    (\"China\",86),\n",
    "    (\"Sweden\",46),\n",
    "    (\"Turkey\",90),\n",
    "    (\"Nepal\",977)))\n",
    "val rdd2 = rdd1.sortByKey()\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones Join en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las operaciones Join nos permiten unir dos RDDs que son probalemente una de las operaciones mas comunes con RDDs clave-valor\n",
    "\n",
    "![title](images\\operation-join.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ages: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[12] at parallelize at <console>:25\r\n",
       "addresses: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[13] at parallelize at <console>:26\r\n",
       "join: org.apache.spark.rdd.RDD[(String, (Int, String))] = MapPartitionsRDD[16] at join at <console>:28\r\n",
       "leftOuterJoin: org.apache.spark.rdd.RDD[(String, (Int, Option[String]))] = MapPartitionsRDD[20] at leftOuterJoin at <console>:31\r\n",
       "rightOuterJoin: org.apache.spark.rdd.RDD[(String, (Option[Int], String))] = MapPartitionsRDD[24] at rightOuterJoin at <console>:34\r\n",
       "fullOuterJoin: org.apache.spark.rdd.RDD[(String, (Option[Int], Option[String]))] = MapPartitionsRDD[28] at fullOuterJoin at <console>:37\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ages = sc.parallelize(List((\"Tom\", 29),(\"John\", 22)))\n",
    "val addresses = sc.parallelize(List((\"James\", \"USA\"), (\"John\", \"UK\")))\n",
    "\n",
    "val join = ages.join(addresses)\n",
    "join.saveAsTextFile(\"out/age_address_join.text\")\n",
    "\n",
    "val leftOuterJoin = ages.leftOuterJoin(addresses)\n",
    "leftOuterJoin.saveAsTextFile(\"out/age_address_left_out_join.text\")\n",
    "\n",
    "val rightOuterJoin = ages.rightOuterJoin(addresses)\n",
    "rightOuterJoin.saveAsTextFile(\"out/age_address_right_out_join.text\")\n",
    "\n",
    "val fullOuterJoin = ages.fullOuterJoin(addresses)\n",
    "fullOuterJoin.saveAsTextFile(\"out/age_address_full_out_join.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mejores Practicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si ambos RDDs tienen claves duplicadas, la operacion Union puede ampliar dramaticamente el tamaño de los datos. Se recomienda ejecutar una operacion *distinct* o *combineByKey* para disminuir el tamaño de las claves si fuera posible\n",
    "- La operacion *Join* puede requerir grandes transferencias de datos o incluso crear conjuntos de datos fuera de nuestras capacidades para administrarlos\n",
    "- Las uniones en general son muy costosas y requieren que las claves de los RDDs esten situadas en la misma particion del disco para que pueda combinarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
