{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion a RDDs Clave-Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Muchos conuuntos de datos que vemos en ejemplos de la vida real suelen ser pares clave-valor\n",
    "- Ejemplos:\n",
    "Un conjunto de datos que contiene numeros de pasaporte y los nombres de los propietarios.\n",
    "- El patron tipico de este tipo de conjunto de datos es que cada fila es una clave que esta asociada a uno o multiples valores\n",
    "- Un RDD clave-valor es un tipo particular de RDD que puede almacenar pares Clave-Valor\n",
    "- Son bloques utiles de construccion en muchos programas Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como crear RDDs clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crear RDDs clave-valor a partir de una lista de datos clave-valor estructurada llamada tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple: (String, Int) = (Juan,25)\r\n",
       "name: String = Juan\r\n",
       "age: Int = 25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creamos una tupla\n",
    "val tuple = (\"Juan\", 25)\n",
    "\n",
    "val name = tuple._1\n",
    "val age = tuple._2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir tuplas en RDDs clave-valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple: List[(String, Int)] = List((Lily,23), (Jack,29), (Mary,29), (James,8))\r\n",
       "pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[2] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Ejemplo 1\n",
    "\n",
    "val tuple = List((\"Lily\", 23), (\"Jack\", 29), (\"Mary\", 29), (\"James\", 8))\n",
    "val pairRDD = sc.parallelize(tuple)\n",
    "\n",
    "pairRDD.coalesce(1).saveAsTextFile(\"out/pair_rdd_from_tuple_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputStrings: List[String] = List(Lily 23, Jack 29, Mary 29, James 8)\r\n",
       "regularRDDs: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at <console>:30\r\n",
       "pairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[6] at map at <console>:32\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Ejemplo 2\n",
    "\n",
    "val inputStrings = List(\"Lily 23\", \"Jack 29\", \"Mary 29\", \"James 8\")\n",
    "val regularRDDs = sc.parallelize(inputStrings)\n",
    "\n",
    "val pairRDD = regularRDDs.map(s => (s.split(\" \")(0), s.split(\" \")(1)))\n",
    "pairRDD.coalesce(1).saveAsTextFile(\"out/pair_rdd_from_regular_rdd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones Filter y MapValue en RDDs clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los RDDs clave-valor pueden utilizar todas las transformaciones disponibles para RDDs regulares, por lo tanto admiten las mismas funciones\n",
    "- Dado que los RDDs clave-valor tienen tuplas, tenemos que ejecutar las funciones que operan en tuplas en lugar de elementos individuales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacion Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La transformacion Filter tambien se puede aplicar a un RDD clave-valor\n",
    "- La transformacion Filter se ejecutaa como una funcion y genera un RDD clave-valor formado por aquellos elementos seleccionados que pasan la funcion filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree un programa Spark para leer los datos del aeropuerto desde in / airports.text; generar un par RDD con el nombre del aeropuerto como clave y el nombre del país como valor. Luego, elimine todos los aeropuertos que se encuentran en Estados Unidos y envíe el par RDD a out / airports_not_in_usa_pair_rdd.text Cada fila del archivo de entrada contiene las siguientes columnas:  Identificación del aeropuerto, nombre del aeropuerto, ciudad principal a la que sirve el aeropuerto, país donde se encuentra el aeropuerto, Código IATA / FAA, Código ICAO, Latitud, Longitud, Altitud, Zona horaria, DST, Zona horaria en formato Olson"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "       (\"Kamloops\", \"Canada\")\n",
    "       (\"Wewak Intl\", \"Papua New Guinea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportsRDD: org.apache.spark.rdd.RDD[String] = input/airports.text MapPartitionsRDD[10] at textFile at <console>:25\r\n",
       "airportPairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[11] at map at <console>:27\r\n",
       "airportsNotInUSA: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at filter at <console>:29\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportsRDD = sc.textFile(\"input/airports.text\")\n",
    "\n",
    "val airportPairRDD = airportsRDD.map(line => (line.split(\",\")(1),\n",
    "      line.split(\",\")(3)))\n",
    "val airportsNotInUSA = airportPairRDD.filter(keyValue => keyValue._2 != \"\\\"United States\\\"\")\n",
    "\n",
    "airportsNotInUSA.saveAsTextFile(\"out/airports_not_in_usa_pair_rdd.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones map y mapValues\n",
    "\n",
    "- Transformaciones map tambien funciona para RDDs clave-valor. Se puede utilizar para convertir un RDD en otro.\n",
    "- Sin embargo, cuando se trabaja con RDDs clave-valor, solo queremos acceder al valor de nuestro par clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio:\n",
    "\n",
    "\n",
    "Cree un programa Spark para leer los datos del aeropuerto desde / airports.text, genere un par RDD con el nombre del aeropuerto siendo la clave y el nombre del país el valor. Luego convierta el nombre del país a mayúsculas y salida del par RDD a out / airports_uppercase.text Cada fila del archivo de entrada contiene las siguientes columnas: Identificación del aeropuerto, nombre del aeropuerto, ciudad principal a la que sirve el aeropuerto, país donde se encuentra el aeropuerto, código IATA / FAA, Código OACI, latitud, longitud, altitud, zona horaria, horario de verano, zona horaria en formato Olson"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ejemplo Salida:\n",
    "\n",
    "       (\"Kamloops\", \"CANADA\")\n",
    "       (\"Wewak Intl\", \"PAPUA NEW GUINEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solucion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportsRDD: org.apache.spark.rdd.RDD[String] = input/airports.text MapPartitionsRDD[25] at textFile at <console>:28\r\n",
       "airportPairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[26] at map at <console>:30\r\n",
       "upperCase: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[27] at mapValues at <console>:33\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportsRDD = sc.textFile(\"input/airports.text\")\n",
    "\n",
    "val airportPairRDD = airportsRDD.map((line: String) => (line.split(\",\")(1),\n",
    "      line.split(\",\")(3)))\n",
    "\n",
    "val upperCase = airportPairRDD.mapValues(countryName => countryName.toUpperCase)\n",
    "\n",
    "upperCase.saveAsTextFile(\"out/airports_uppercase.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
